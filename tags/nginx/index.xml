<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nginx on Oopschen的日志</title>
    <link>http://oopschen.github.io/tags/nginx/</link>
    <description>Recent content in Nginx on Oopschen的日志</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>版权归Ray所有, 授权在MIT协议下.</copyright>
    <lastBuildDate>Sun, 01 Sep 2013 19:38:24 +0000</lastBuildDate>
    
	<atom:link href="http://oopschen.github.io/tags/nginx/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Nginx - CPU Cacheline 深思</title>
      <link>http://oopschen.github.io/posts/2013/cpu-cacheline/</link>
      <pubDate>Sun, 01 Sep 2013 19:38:24 +0000</pubDate>
      
      <guid>http://oopschen.github.io/posts/2013/cpu-cacheline/</guid>
      <description>在nginx的内存管理,我们会看到*ngx_align_ptr*这样的代码,从命名上来看是对指针地址的对齐操作。为什么要这么做呢？不是增加负担么？带着这样的问题,我们来看看CPU cache这个很有深度的问题。这篇博客是对drepper的理解和消化,有英文阅读能力的可以直接参考原文,跳过此博客。（PS：人家2007年写的,惭愧惭愧)
内存硬件操作 这里的内存只是对同步的DRAM进行表述,对于其他的内存类型不适用。
内存的所有操作都是由内存控制器完成,而当下内存采用行列模式,管理硬件内存地址。也就是说,当内存控制器读取或写入数据的时候,会有3步操作:
1. precharge, 对内存充电操作
2. RAS, 对行的选择
3. CAS, 对列的选择
最后才能开始读取或写入这个单元的数据, 然而这几个操作相对cpu的运行速度是很慢很慢的。所以出于充分利用cpu的目的,硬件发展中在cpu中加入了L1,L2,L3的缓存。每级缓存的访问速度是递减的,而容量是递增的,他们的访问速度都是内存的10倍以上。这样CPU不用等待缓慢的内存数据,而直接从高速的缓存中取。也就是说缓存的命中率决定CPU的效率。同时,对内存的读取和写入也不视一个字节一个字节写入,而是一串字节,我们通常称为cache line。这样的设计是基于相邻的东西更有可能被使用的原则,这样也就提高了CPU的利用率。
Cache line 首先我们来看看Cache line的结构：
1. tag,用于区分不同内存地址的标签
2. set,用于在tag中选择不同的集合
3. offseet, 用于一个cache line中开始的位置
如上面说内存每访问一个地址都会一并把cache line大小的数据从ram中读取到缓存中。那么,当cpu读取数据的时候首先会从缓存中取,而这个取的速度必须很快,不然就失去了缓存的意义。因此,缓存的大小都很小,这也是出于对速度的要求,因为对大量的缓存比对需要很多时间。
这个时候人们总会用索引的方式,加快比对,这也就是上述结构的意义。CPU用set过滤出一小部分的地址,然后用offset和tag比对是否为请求的数据。
通过cache line的实现,CPU实现了预提取数据的功能。当然如果那个原则被打破,这样的设计也是低效的。
实践 理论虽如此,我们来看看实际代码表现如何？
cache line大小 首先要解决的问题就是获取CPU cache line的大小。我们以linux为例：
cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size  这个文件记录了CPU的cache line大小,单位是字节。这个文件的目录下还存在着一些其他文件
 coherency_line_size
level
number_of_sets
physical_line_partition
shared_cpu_list
shared_cpu_map
size
type
ways_of_associativity
 这些文件分别说明了缓存的类型,等级,几路set和共享缓存的CPU等等信息
代码论证 既然缓存只是提高了CPU获取速度的速度,那么我们可以设计如下的代码来检验这个理论。
1. CPU按字节对内存进行访问
2. 不同的代码分配到不同的CPU核上去,减少系统调度带来的干扰
3. 对比组为缓存对齐的指针和随机的指针
缓存对齐的代码 #define _GNU_SOURCE  #include &amp;lt;sched.</description>
    </item>
    
    <item>
      <title>Dig Nginx - Nginx 源代码初见</title>
      <link>http://oopschen.github.io/posts/2013/nginx-overview/</link>
      <pubDate>Fri, 02 Aug 2013 15:14:19 +0000</pubDate>
      
      <guid>http://oopschen.github.io/posts/2013/nginx-overview/</guid>
      <description>最近有点时间,看了下nginx源代码,先对这部分内容做点小札记。
观后感 在没有文档帮助的情况下,我大约花费4天的时间来摸清nginx是怎么工作的(c语言的经验在半年左右)。显然,nginx的源码对于我来说不是那么友好,主要体现在如下方面：
1.函数的代码长度。在nginx的源代码里,阅读到了800+的一段函数,这情何以堪。
2.宏名称大小写。因为个人习惯,宏都是大写的,而nginx中有许多宏都是小写。当然其中有些宏在不同的设置下确实是函数,但我以为还是大写比较容易识别。
3.模块化的全局变量。所谓模块化就是,其实是在编译期间,对全局变量的赋值,包括变量,函数句柄等。而这些变量又是全局的,使得阅读代码的难度增加。
当然,在其中也汲取到了不少东西：
1. 利用cpu的cache line优化内存访问
2. 局部内存池
模块化 nginx的模块化可以说是伪模块化,它不是在运行时期可以添加或删除的模块,而是在编译时期,配置的模块化,当然这也是出于效率考虑。nginx的模块配置由自动生成的ngx_modules.c(源代码目录/objs/ngx_modules.c)决定,这个文件主要定义了全局变量*ngx_modules*这个数组。数组的元素是类型为*ngx_module_t*的全局变量。
ngx_module_t类型包含如下几个元素：
1. 模块名称
2. 模块编号
3. 模块上下文编号
4. 模块生命周期里所需的函数指针
5. 模块类型
6. 模块命令,在文件配置文件解析时,用于设置变量的值
7. 模块上下文,用于配置和保存worker时期的变量值
nginx 初始化 以下是nginx初始化的内容,略过一些检查变量值的步骤： 1. 初始化cycle。cycle包含整个nginx的配置文件信息。其中包含很多的步骤,包括初始化模块,解析配置文件,处理listen的老端口等等。 2. 根据配置文件fork worker进程,每个worker进程都会有各自的epoll,这和我当时预期的不太一致
3. 主进程负责响应用户的命令,包括重启,关闭,启动等等
3. fork 出来的worker进程则开启poll句柄
4. 将配置文件中的listen的端口加入到poll句柄中轮询
5. 当监听的端口被访问的时候,就进入了一般的服务器解析的过程,当然在这个过程中,这个worker进程被独占。
具体代码 说了这些概念性的总结话语,还是来看看代码比较实际,下面的代码追踪是基于linux的epoll配置进行的：
core/nginx.c 所有一切的开端,这中包含了main函数。其中值得看的是两个函数：
1. ngx_os_init, 根据不同的cpu初始化cpu cache line的大小 2. ngx_init_cycle, 初始化cycle 最后我们就到了ngx_master_process_cycle, 处理cycle的步骤
ngx_init_cycle (core/ngx_cycle.c) 这其中的内容可以大致看过,等到后面的步骤可以细看哪些变量被初始化。
ngx_master_process_cycle (os/unix/ngx_process_cycle.c)  ngx_start_worker_processes, 负责fork出worker进程 ngx_start_cache_manager_processes, 负责fork出cache进程   ngx_start_worker_processes (os/unix/ngx_process_cycle.</description>
    </item>
    
  </channel>
</rss>